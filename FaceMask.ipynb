{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!wget https://data-flair.s3.ap-south-1.amazonaws.com/Data-Science-Data/face-mask-dataset.zip\n",
    "!unzip /content/face-mask-dataset.zip\n",
    "\n",
    "!unzip /content/Dataset/train.zip\n",
    "!unzip /content/Dataset/test.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mask_train_dir = '/content/train/with_mask'\n",
    "without_mask_train_dir = '/content/train/without_mask'\n",
    "\n",
    "test_dir = '/content/test'\n",
    "\n",
    "mask_train = os.listdir('/content/train/with_mask')\n",
    "without_mask_train = os.listdir('/content/train/without_mask')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample Image output\n",
    "sample_image_mask = os.path.join(mask_train_dir, mask_train[1])\n",
    "image_mask = cv2.imread(sample_image_mask)\n",
    "image_mask = cv2.cvtColor(image_mask, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(img_names, data_dir, model_size=None):\n",
    "    \"\"\"Loads images in a 4D array.\n",
    "\n",
    "    Args:\n",
    "        img_names: A list of images names.\n",
    "        model_size: The input size of the model.\n",
    "        data_dir: Directory to load data from\n",
    "\n",
    "    Returns:\n",
    "        A 4D NumPy array.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img = os.path.join(data_dir, img_name)\n",
    "        img = cv2.imread(img)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, dsize=model_size, interpolation = cv2.INTER_AREA)\n",
    "        img = np.array(img)\n",
    "        #img = np.expand_dims(img, axis=0)\n",
    "        imgs.append(img)\n",
    "\n",
    "    imgs = np.array(imgs, dtype=\"float32\")\n",
    "\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_mask = load_images(mask_train, mask_train_dir, (224, 224))\n",
    "without_mask = load_images(without_mask_train, without_mask_train_dir, (224, 224))\n",
    "\n",
    "imgs = np.concatenate((with_mask, without_mask))\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_mask = np.ones(shape=(with_mask.shape[0], 1), dtype = np.int32)\n",
    "labels_without_mask = np.zeros(shape=(without_mask.shape[0], 1), dtype = np.int32)\n",
    "\n",
    "labels = np.concatenate((labels_mask, labels_without_mask))\n",
    "\n",
    "# Convert Labels to one-hot encoding\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "labels = to_categorical(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train-data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, validation_x, train_y, validation_y = train_test_split(imgs, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.utils import np_utils \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, Input\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_detector(input_shape, num_classes):\n",
    "\n",
    "    inputs = Input(input_shape)\n",
    "\n",
    "    net = Conv2D(64, (3,3), padding = 'same')(inputs)\n",
    "    net = BatchNormalization(axis = 3)(net)\n",
    "    net = Activation('relu')(net)\n",
    "    net = MaxPooling2D((2,2))(net)\n",
    "  \n",
    "    net = Conv2D(128, (3,3), padding = 'same')(inputs)\n",
    "    net = BatchNormalization(axis = 3)(net)\n",
    "    net = Activation('relu')(net)\n",
    "    net = MaxPooling2D((2,2))(net)\n",
    "  \n",
    "    net = Flatten()(net)\n",
    "    net = Dense(64, activation = 'relu')(net)\n",
    "    net = Dropout(0.5)(net)\n",
    "\n",
    "    net = Dense(num_classes, activation = 'softmax')(net)\n",
    "  \n",
    "    model = Model(inputs = inputs, outputs = net, name = 'Face_mask_detection')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"FaceDtection\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 128)     3584      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 224, 224, 128)     512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 224, 224, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1605632)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                102760512 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 102,764,738\n",
      "Trainable params: 102,764,482\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "\n",
    "model = mask_detector(input_shape, 2)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the optimizer and compile the model\n",
    "optimizer = Adam(lr=0.01, decay=5e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopper = EarlyStopping(patience=15, verbose=1)\n",
    "checkpointer = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
    "callback_list = [checkpointer, earlystopper]\n",
    "\n",
    "data_augmentation = True\n",
    "num_epochs = 10\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    h = face_mask_model.fit(train_x, train_y,\n",
    "                  batch_size = 64,\n",
    "                  validation_data = (validation_x, validation_y),\n",
    "                  callbacks = callback_list, \n",
    "                  epochs = num_epochs,\n",
    "                  verbose = 1)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None)\n",
    "\n",
    "    # Compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(train_x)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    results = face_mask_model.fit_generator(datagen.flow(train_x, train_y,\n",
    "                                     batch_size = 64),\n",
    "                                     validation_data = (validation_x, validation_y),\n",
    "                                     epochs = num_epochs, verbose = 1, workers = 4,\n",
    "                                     callbacks = callback_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "print(results.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(results.history['accuracy'])\n",
    "plt.plot(results.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0728 19:33:42.202229 140606702257984 deprecation.py:323] From /home/vaibhav/my_project_dir/my_project_env/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "## Load Model\n",
    "model = load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_detect(faces, frame):\n",
    "    \n",
    "    for i in range(len(faces)):\n",
    "        x, y, w, h = faces[i]\n",
    "        face_frame = frame[y:y+h,x:x+w]\n",
    "        face_frame = cv2.cvtColor(face_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_frame = cv2.resize(face_frame, (224, 224))\n",
    "        face_frame = np.array(face_frame)\n",
    "        face_frame = np.expand_dims(face_frame, axis=0)\n",
    "        face_frame =  preprocess_input(face_frame)\n",
    "        \n",
    "        preds = model.predict(face_frame)\n",
    "        for pred in preds:\n",
    "            (withoutMask, mask) = pred\n",
    "        label = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
    "        color = (0, 255, 0) if label == \"Mask\" else (0, 0, 255)\n",
    "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
    "        cv2.putText(frame, label, (x, y- 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
    "    \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h),color, 2)\n",
    "    \n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful\n"
     ]
    }
   ],
   "source": [
    "front_face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "#profile_face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_profileface.xml')\n",
    "\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if camera opened successfully\n",
    "if (video_capture.isOpened()== False): \n",
    "    print(\"Error opening video stream or file\")\n",
    "else:\n",
    "    print(\"Successful\")\n",
    "    \n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    faces = front_face_cascade.detectMultiScale(gray,\n",
    "                                          scaleFactor=1.1,\n",
    "                                          minNeighbors=5,\n",
    "                                          minSize=(60, 60),\n",
    "                                          flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    frame = face_detect(faces, frame)\n",
    "        \n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
